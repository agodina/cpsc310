import csv
import random
from random import randint
from tabulate import tabulate
from math import log
from collections import Counter
import numpy as np
import re


def read_csv(filename):
    the_file = open(filename, 'r')  # opens the file
    the_reader = csv.reader(the_file, dialect='excel')
    table = []  # makes an empty table
    for row in the_reader:
        if len(row) > 0:
            table.append(row)  # adds each row to the table
    the_file.close()  # closes the file
    return table  # returns the new table

def file_to_list(filename):
    #Puts the file into a 2D list
    table = []
    f = open(filename, 'r')
    for row in f:
        my_list = re.split(',|\n', row)
        good = True
        for token in my_list: #removes any row with missing values
            if token == 'NA':
                good = False
        if good:
            table.append(my_list)
    return table

def get_column(table, index):
    column = []  # creates a new table
    for row in table:
        column.append(row[index])
    return column

def attribute_frequencies(instances, att_index, class_index):
    # get unique list of attribute and class values
    att_vals = list(set(get_column(instances, att_index)))
    class_vals = list(set(get_column(instances, class_index)))
    # initialize the result
    result = {v: [{c: 0 for c in class_vals}, 0] for v in att_vals}  # build up the frequencies
    for row in instances:
        label = row[class_index]
        att_val = row[att_index]
        result[att_val][0][label] += 1
        result[att_val][1] += 1
    return result

def calc_estart(instances, att_index):
    size = len(instances)
    Yes = 0.0
    No = 0.0

    for row in instances:
        if row[att_index] == 'yes':
            Yes += 1
        else:
            No += 1

    p, q = Yes / size, No / size
    e_start = -(p * log(p, 2)) - (q * log(q, 2))

    return e_start


def calc_enew(instances, att_index, class_index):
    # get the length of the partition
    D = len(instances)
    # calculate the partition stats for att_index (see below)
    freqs = attribute_frequencies(instances, att_index, class_index)
    # find E_new from freqs (calc weighted avg)
    E_new = 0
    for att_val in freqs:
        D_j = float(freqs[att_val][1])
        probs = []
        for (_, c) in freqs[att_val][0].items():
            if c != 0:
                probs.append(c / D_j)
        #probs = [(c / D_j) for (_, c) in freqs[att_val][0].items()]
        E_D_j = -sum([p * log(p, 2) for p in probs])
        E_new += (D_j / D) * E_D_j
    return E_new


def get_best_index(table, indexes):
    e_news = []
    for att in indexes:
        e_start = calc_estart(table, 3)
        e_new = calc_enew(table, att, 3)
        #print('e_start is: ' + str(e_start) + ' e_new is: ' + str(e_new))
        e_news.append(e_start - e_new)
    return indexes[e_news.index(max(e_news))]

def group_by(table, att_index):
    # create unique list of grouping values
    grouping_values = list(set(get_column(table, att_index)))
    # create list of n empty partitions
    result = []
    for val in grouping_values:
        result.append([])
    # add rows to each partition
    for row in table:
        result[grouping_values.index(row[att_index])].append(row[:])
    return result

def random_attribute_subset(attributes, F):
    # shuffle and pick first F
    shuffled = attributes[:] # make a copy
    random.shuffle(shuffled)
    return shuffled[:F]


def split_tables(table, atts, final):
    if len(atts) < 1:
        return
    this_atts = atts[:]
    this_atts = random_attribute_subset(this_atts, 2)#get a random sub set of this_atts
    best = get_best_index(table, this_atts)
    this_atts.remove(best)
    grouped = group_by(table, best)
    for item in grouped:
        if len(atts) == 1: #this checks if it will be a leaf
            final.append(item)
    for item in grouped:
        split_tables(item, this_atts, final)
    return final

def split_best_tables(table, atts, final):
    if len(atts) < 1:
        return
    this_atts = atts[:]
    best = get_best_index(table, this_atts)
    this_atts.remove(best)
    grouped = group_by(table, best)
    for item in grouped:
        if len(atts) == 1: #this checks if it will be a leaf
            final.append(item)
    for item in grouped:
        split_best_tables(item, this_atts, final)
    return final

def Stratified(table):
    randomized = table[:]  # this will randomize the table we passed in
    n = len(table)
    for i in range(n):
        j = randint(0, n - 1)
        randomized[i], randomized[j] = randomized[j], randomized[i]  # changes all the positions of the item in the list

    k = 3
    empty = [[] for i in range(k)]  # the empty 10-folds
    count = 0
    temp = randomized
    count = 0
    for row in table:
        if count == k:
            count = 0
        empty[count].append(row)
        count += 1
    training_set = empty[0] + empty[1]
    return [training_set, empty[2]]

def get_forest(table, N):
    forrest = []
    class_tables = []
    for i in range(0, N): #this loop holds the creation of the tree
        class_table = []
        totalAcc = 0.0
        TP = 0.0
        NTrainingAndTest = Stratified(table) #split up the table 2:1
        final = []
        if N == 1:
            final = split_best_tables(NTrainingAndTest[0], [0, 1, 2], final)  # produces a decision treee
        else:
            final = split_tables(NTrainingAndTest[0], [0, 1, 2], final) #produces a decision treee
        for row in final:
            countYes = 0.0
            countNo = 0.0
            for token in row:
                if len(token) > 2:
                    if token[3] == 'yes':
                        countYes += 1.0
                    else:
                        countNo += 1.0
            row.append([countYes / (len(row) * 1.0)])
            row.append([countNo / (len(row) * 1.0)])
        for item in NTrainingAndTest[1]:
            for row in final:
                prediction = 'yes' # the default is no
                for instance in row:
                    if len(instance) > 2:
                        if instance[0] == item[0] and instance[1] == item[1] and instance[2] == item[2]:
                            probs = [row[len(row) - 2], row[len(row) - 1]]
                            if probs[0] > probs[1]:
                                prediction = 'yes'
                            else:
                                prediction = 'no'
            realRank = item[3]
            # print(prediction + ' : ' + realRank)
            if prediction == realRank:
                TP += 1.0
            class_table.append([(prediction), (realRank)])
        accuracy = TP / len(NTrainingAndTest[1]) * 1.0
        totalAcc += accuracy
        #print(TP)
        #print(totalAcc)
        #print(len(NTrainingAndTest[1]))
        final.append([totalAcc])
        forrest.append(final)
        class_tables.append(class_table)
    return forrest, class_tables

def get_key(item):
    return item[len(item) - 1][0]

def get_top_trees(forrest, M):
    od_forrest = sorted(forrest, key=get_key, reverse=True)
    return od_forrest[0:M]

def make_predict(forrest, test):
    predictions = []
    accuracy = 0.0
    TP = 0.0
    for item in test:
        for tree in forrest:
            for row in tree:
                prediction = 'yes'  # the default is no
                if len(row) > 2:
                    for instance in row:
                        if len(instance) > 2:
                            if instance[0] == item[0] and instance[1] == item[1] and instance[2] == item[2]:
                                probs = [row[len(row) - 2], row[len(row) - 1]]
                                if probs[0] > probs[1]:
                                    prediction = 'yes'
                                else:
                                    prediction = 'no'
            # print(prediction + ' : ' + realRank)
            predictions.append(prediction)
        data = Counter(predictions)
        #data.most_common()  # Returns all unique items and their counts
        #print(data.most_common(1)[0][0])  # Returns the highest occurring item
        if data.most_common(1)[0][0] == item[3]:
            TP += 1.0
    accuracy = TP / len(test) * 1.0
    return accuracy

def get_best_index2(table, indexes):
    e_news = []
    for att in indexes:
        e_start = estart_car(table, 0)
        e_new = calc_enew(table, att, 0)
        e_news.append(e_start - e_new)
    return indexes[e_news.index(max(e_news))]

def estart_car(instances, index):
    empty = []
    count = 0.0
    for i in range(1,10):
        for spot in instances:
            if spot[index] == i:
                count +=1
        empty.append(count)
        count =0.0

    e_start = 0.0
    size = float(len(instances))
    for j in empty:
        if j == 0.0:
            break
        else:
            e_start += -((j/size) * log((j/size),2))
    return e_start

def split_tables2(table, atts, final):
    if len(atts) < 1:
        return
    this_atts = atts[:]
    best = get_best_index2(table, this_atts)
    this_atts.remove(best)
    grouped = group_by(table, best)
    for item in grouped:
        if len(atts) == 1: #this checks if it will be a leaf
            final.append(item)
    for item in grouped:
        split_tables2(item, this_atts, final)
    return final

def DOE_class(mpg):
    mpg = float(mpg)
    guessRank = -1
    if mpg >= 45.0:
        guessRank = 10
    if  mpg < 45.0 and mpg >= 37.0:
        guessRank = 9
    if mpg < 37.0 and mpg >= 31.0:
        guessRank = 8
    if mpg < 31.0 and mpg >= 27.0:
        guessRank = 7
    if mpg < 27.0 and mpg >= 24.0:
        guessRank = 6
    if mpg < 24.0 and mpg >= 20.0:
        guessRank = 5
    if mpg < 20.0 and mpg >= 17.0:
        guessRank = 4
    if mpg < 17.0 and mpg >= 15.0:
        guessRank = 3
    if mpg < 15.0 and mpg >= 13.0:
        guessRank = 2
    if mpg < 13.0:
        guessRank = 1
    return guessRank

def weight_class(weight):
    weight = int(weight)
    guessWeight = -1
    if weight >= 3500:
        guessWeight = 5
    if weight <= 3499 and weight >= 3000:
        guessWeight = 4
    if weight <= 2999 and weight >= 2500:
        guessWeight = 3
    if weight <= 2499 and weight >= 2000:
        guessWeight = 2
    if weight <= 1999:
        guessWeight = 1
    return guessWeight

def change_mpg(instances, att_index):
    if att_index == 0:
        for row in instances:
            row[att_index] = DOE_class(row[att_index])
    else:
        for row in instances:
            row[att_index] = weight_class(row[att_index])
    return instances

def add_sum_col(table):
    #calculates total for tabulate data
    for row in table:
        row.append(sum_row(row))
    return table

def sum_row(row):
    # prints the sum of the rows without the first column
    return sum([row[i + 1] for i in range(len(row) - 1)])


def frequencies(table, index):
    """Returns a unique, sorted list of values in xs and occurrence counts for each value"""
    xs = sorted(get_column(table, index))
    values, counts = [], []
    for x in xs:
        if x not in values:
            values.append(x)
            counts.append(1)
        else:
            counts[-1] += 1
    return values, counts

def return_class(value):
    ind = np.argmax(value[1])
    return ind

def auto_confusion(table):
    # initializes the table
    confusion_table = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]
    for row in confusion_table:
        for i in range(10):
            row.append(0)

    #fill in table with prediction vs actual
    for row in table:
        actual = row[0]
        pred = row[1]
        confusion_table[actual-1][pred]+= 1

    #adds totals column
    add_sum_col(confusion_table)

    #adds recognition column
    i = 1
    for row in confusion_table:
        x = confusion_table[i-1][i]
        if x ==0 or row[11] == 0:
            row.append(0)
        else:
            reg = 100 * float(x)/float(row[11])
            row.append(reg)
        i += 1
    print ('===================================================================================')
    print ('Auto Data: Confusion Matrices')
    print ('===================================================================================')
    print ('Linear Regression(Stratified 10-fold Cross Validation Results):')
    print (tabulate(confusion_table, headers=["MPG", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "Total", "Recognition(%)"]))

def auto_Stratified(table):
    randomized = table[:]  # this will randomize the table we passed in
    n = len(table)
    for i in range(n):
        j = randint(0, n - 1)
        randomized[i], randomized[j] = randomized[j], randomized[i]  # changes all the positions of the item in the list

    k = 10
    empty = [[] for i in range(k)]  # the empty 10-folds
    count = 0
    temp = randomized
    count = 0
    for row in table:
        if count == 10:
            count = 0
        empty[count].append(row)
        count += 1

    class_table = []
    totalAcc = 0

    for i in range(0, k):
        training = []
        training = empty[(i + 1) % 10] + empty[(i + 2) % 10] + empty[(i + 3) % 10] + empty[(i + 4) % 10] + empty[
            (i + 5) % 10] + empty[(i + 6) % 10] + empty[(i + 7) % 10] + empty[(i + 8) % 10] + empty[(i + 9) % 10] + \
                   empty[(i + 10) % 10]
        TP = 0.0
        atts2 = [1, 4, 6]
        final = []
        final = split_tables2(training, atts2, final)
        for row in final:
            val = frequencies(row, 0)
            x = return_class(val)
            row.append(val[0][x])
        for item in empty[i]:
            for row in final:
                if row[0][1] == item[1] and row[0][4] == item[4] and row[0][6] == item[6]:
                    prediction = row[len(row)-1]
            realRank = item[0]
            if prediction == realRank:
                TP += 1.0
            class_table.append([(prediction), (realRank)])
        accuracy = TP / len(empty[i]) * 1.0
        totalAcc += accuracy
    totalAcc = totalAcc / (k * 1.0)

    print ('Stratified 10-Fold Cross Validation')
    print('Accuracy: ' + str(totalAcc) + ', Error Rate: ' + str(1 - totalAcc))
    return class_table


def step_2(table):
    firstTrainingAndTest = Stratified(table)
    forrest = get_forest(firstTrainingAndTest[1], 20)
    top_forrest = get_top_trees(forrest[0], 7)
    acc = make_predict(top_forrest, firstTrainingAndTest[1])
    print('Accuracy of forrest: ' + str(acc))

    normal = get_forest(firstTrainingAndTest[1], 1)
    acc = make_predict(normal[0], firstTrainingAndTest[1])
    print('Accuracy of normal tree: ' + str(acc))

def main():
    tableMaster = read_csv('titanic.txt')
    ''''
    firstTrainingAndTest = Stratified(tableMaster)
    final = []
    final = split_tables(firstTrainingAndTest[0], [0, 1, 2], final)
    print('')
    forrest = get_forest(firstTrainingAndTest[1], 3)
    top_forrest = get_top_trees(forrest, 2)
    make_predict(top_forrest, firstTrainingAndTest[1])
    '''
    step_2(tableMaster)

    print('Auto Data:')
    master = file_to_list('prof-auto-data.txt')
    masterTable1 = change_mpg(master, 0)
    masterTable = change_mpg(masterTable1, 4)

    atts2 = [1, 4, 6]
    car = []
    car = split_tables2(masterTable, atts2, car)
    for row in car:
        val = frequencies(row, 0)
        x = return_class(val)
        row.append(val[0][x])
    i = auto_Stratified(masterTable)
    auto_confusion(i)
    print('')




if __name__ == '__main__':
    main()
