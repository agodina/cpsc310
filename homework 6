import csv
import random
from random import randint
from tabulate import tabulate
from math import log
from collections import Counter
import numpy as np
import re


def read_csv(filename):
    the_file = open(filename, 'r')  # opens the file
    the_reader = csv.reader(the_file, dialect='excel')
    table = []  # makes an empty table
    for row in the_reader:
        if len(row) > 0:
            table.append(row)  # adds each row to the table
    the_file.close()  # closes the file
    return table  # returns the new table

def file_to_list(filename):
    #Puts the file into a 2D list
    table = []
    f = open(filename, 'r')
    for row in f:
        my_list = re.split(',|\n', row)
        good = True
        for token in my_list: #removes any row with missing values
            if token == 'NA':
                good = False
        if good:
            table.append(my_list)
    return table

def get_column(table, index):
    column = []  # creates a new table
    for row in table:
        column.append(row[index])
    return column

def attribute_frequencies(instances, att_index, class_index):
    # get unique list of attribute and class values
    att_vals = list(set(get_column(instances, att_index)))
    class_vals = list(set(get_column(instances, class_index)))
    # initialize the result
    result = {v: [{c: 0 for c in class_vals}, 0] for v in att_vals}  # build up the frequencies
    for row in instances:
        label = row[class_index]
        att_val = row[att_index]
        result[att_val][0][label] += 1
        result[att_val][1] += 1
    return result

def calc_estart(instances, att_index):
    size = len(instances)
    Yes = 0.0
    No = 0.0
    for row in instances:
        if row[att_index] == 'yes':
            Yes += 1
        else:
            No += 1

    p, q = Yes / size, No / size
    e_start = -(p * log(p, 2)) - (q * log(q, 2))

    return e_start


def calc_enew(instances, att_index, class_index):
    # get the length of the partition
    D = len(instances)
    # calculate the partition stats for att_index (see below)
    freqs = attribute_frequencies(instances, att_index, class_index)
    # find E_new from freqs (calc weighted avg)
    E_new = 0
    for att_val in freqs:
        D_j = float(freqs[att_val][1])
        probs = []
        for (_, c) in freqs[att_val][0].items():
            if c != 0:
                probs.append(c / D_j)
        #probs = [(c / D_j) for (_, c) in freqs[att_val][0].items()]
        E_D_j = -sum([p * log(p, 2) for p in probs])
        E_new += (D_j / D) * E_D_j
    return E_new


def get_best_index(table, indexes):
    e_news = []
    for att in indexes:
        e_start = calc_estart(table, 3)
        e_new = calc_enew(table, att, 3)
        #print('e_start is: ' + str(e_start) + ' e_new is: ' + str(e_new))
        e_news.append(e_start - e_new)
    return indexes[e_news.index(max(e_news))]

def group_by(table, att_index):
    # create unique list of grouping values
    grouping_values = list(set(get_column(table, att_index)))
    # create list of n empty partitions
    result = []
    for val in grouping_values:
        result.append([])
    # add rows to each partition
    for row in table:
        result[grouping_values.index(row[att_index])].append(row[:])
    return result

def random_attribute_subset(attributes, F):
    # shuffle and pick first F
    shuffled = attributes[:] # make a copy
    random.shuffle(shuffled)
    return shuffled[:F]


def split_tables(table, atts, atts_dom, final):
    if len(atts) < 1:
        return
    this_atts = atts[:]
    this_atts = random_attribute_subset(this_atts, 2)#get a random sub set of this_atts
    best = get_best_index(table, this_atts)
    this_atts.remove(best)
    grouped = group_by(table, best)
    complete = True
    if len(grouped) < len(atts_dom[best]):
        complete = False
    if complete:
        for item in grouped:
            if len(atts) == 1: #this checks if it will be a leaf
                final.append(item)
        '''
        If one of the groups is empty, stop
        '''
        for item in grouped:
            split_tables(item, this_atts, atts_dom, final)
    else:
        for item in table:
            item.insert(best, 'NA')
            item.remove(item[best + 1])
        final.append(table)
    return final

def split_best_tables(table, atts, atts_dom, final):
    if len(atts) < 1:
        return
    this_atts = atts[:]
    best = get_best_index(table, this_atts)
    this_atts.remove(best)
    grouped = group_by(table, best)
    complete = True
    if len(grouped) < len(atts_dom[best]):
        complete = False
    if complete:
        for item in grouped:
            if len(atts) == 1: #this checks if it will be a leaf
                final.append(item)
        '''
        If one of the groups is empty, stop
        '''
        for item in grouped:
            split_best_tables(item, this_atts, atts_dom, final)
    else:
        for item in table:
            item.insert(best, 'NA')
            item.remove(item[best + 1])
        final.append(table)
    return final

def split_tables2(table, atts, atts_dom, final):
    if len(atts) < 1:
        return
    this_atts = atts[:]
    this_atts = random_attribute_subset(this_atts, 2)#get a random sub set of this_atts
    best = get_best_index2(table, this_atts)
    this_atts.remove(best)
    grouped = group_by(table, best)
    complete = True
    dom_index = 0
    if best == 1:
        dom_index = 0
    elif best == 4:
        dom_index = 1
    elif best == 6:
        dom_index = 2
    if len(grouped) < len(atts_dom[dom_index]):
        complete = False
    if complete:
        for item in grouped:
            if len(atts) == 1: #this checks if it will be a leaf
                final.append(item)
        '''
        If one of the groups is empty, stop
        '''
        for item in grouped:
            split_tables2(item, this_atts, atts_dom, final)
    else:
        for item in table:
            item.insert(best, 'NA')
            item.remove(item[best + 1])
        final.append(table)
    return final

def split_best_tables2(table, atts, atts_dom, final):
    if len(atts) < 1:
        return
    this_atts = atts[:]
    best = get_best_index2(table, this_atts)
    this_atts.remove(best)
    grouped = group_by(table, best)
    complete = True
    dom_index = 0
    if best == 1:
        dom_index = 0
    elif best == 4:
        dom_index = 1
    elif best == 6:
        dom_index = 2
    if len(grouped) < len(atts_dom[dom_index]):
        complete = False
    if complete:
        for item in grouped:
            if len(atts) == 1: #this checks if it will be a leaf
                final.append(item)
        '''
        If one of the groups is empty, stop
        '''
        for item in grouped:
            split_best_tables2(item, this_atts, atts_dom, final)
    else:
        for item in table:
            item.insert(best, 'NA')
            item.remove(item[best + 1])
        final.append(table)
    return final

def split_tables_wisc(table, atts, atts_dom, final):
    if len(atts) < 1:
        return
    this_atts = atts[:]
    this_atts = random_attribute_subset(this_atts, 2)#get a random sub set of this_atts
    best = get_best_index2(table, this_atts)
    this_atts.remove(best)
    grouped = group_by(table, best)
    complete = True
    if len(grouped) < len(atts_dom[best]):
        complete = False
    if complete:
        for item in grouped:
            if len(atts) == 1: #this checks if it will be a leaf
                final.append(item)
        '''
        If one of the groups is empty, stop
        '''
        for item in grouped:
            split_tables_wisc(item, this_atts, atts_dom, final)
    else:
        for item in table:
            item.insert(best, 'NA')
            item.remove(item[best + 1])
        final.append(table)
    return final

def split_best_tables_wisc(table, atts, atts_dom, final):
    if len(atts) < 1:
        return
    this_atts = atts[:]
    best = get_best_index2(table, this_atts)
    this_atts.remove(best)
    grouped = group_by(table, best)
    complete = True
    if len(grouped) < len(atts_dom[best]):
        complete = False
    if complete:
        for item in grouped:
            if len(atts) == 1: #this checks if it will be a leaf
                final.append(item)
        '''
        If one of the groups is empty, stop
        '''
        for item in grouped:
            split_best_tables_wisc(item, this_atts, atts_dom, final)
    else:
        for item in table:
            item.insert(best, 'NA')
            item.remove(item[best + 1])
        final.append(table)
    return final


def Stratified(table):
    randomized = table[:]  # this will randomize the table we passed in
    n = len(table)
    for i in range(n):
        j = randint(0, n - 1)
        randomized[i], randomized[j] = randomized[j], randomized[i]  # changes all the positions of the item in the list

    k = 3
    empty = [[] for i in range(k)]  # the empty 10-folds
    count = 0
    temp = randomized
    count = 0
    for row in table:
        if count == k:
            count = 0
        empty[count].append(row)
        count += 1
    training_set = empty[0] + empty[1]
    return [training_set, empty[2]]

def find_leaf(tree, instance):
    scores = []
    for leaf in tree:
        score = 0.0
        if len(leaf) > 1:
            for i in range(0, 3):
                if leaf[0][i] != 'NA':
                    if leaf[0][i] == instance[i]:
                        score += 1.0
                    else:
                        score -= 1.0
        scores.append(score)
    return tree[scores.index(max(scores))]

def get_forest(table, N):
    forrest = []
    class_tables = []
    for i in range(0, N): #this loop holds the creation of the tree
        class_table = []
        totalAcc = 0.0
        TP = 0.0
        NTrainingAndTest = Stratified(table) #split up the table 2:1
        final = []
        if N == 1:
            final = split_best_tables(NTrainingAndTest[0], [0, 1, 2], [['crew', 'first', 'second', 'third'], ['adult', 'child'], ['male', 'female']], final)  # produces a decision treee
        else:
            final = split_tables(NTrainingAndTest[0], [0, 1, 2], [['crew', 'first', 'second', 'third'], ['adult', 'child'], ['male', 'female']], final) #produces a decision treee
        for row in final:
            countYes = 0.0
            countNo = 0.0
            for token in row:
                if len(token) > 2:
                    if token[3] == 'yes':
                        countYes += 1.0
                    else:
                        countNo += 1.0
            row.append([countYes / (len(row) * 1.0)])
            row.append([countNo / (len(row) * 1.0)])
        #NTrainingAndTest[1] = NTrainingAndTest[1][1:3]
        for item in NTrainingAndTest[1]:
            leaf = find_leaf(final, item)
            probs = [leaf[len(leaf) - 2], leaf[len(leaf) - 1]]
            if probs[0] > probs[1]:
                prediction = 'yes'
            else:
                prediction = 'no'
            realRank = item[3]
            # print(prediction + ' : ' + realRank)
            if prediction == realRank:
                TP += 1.0
            class_table.append([(prediction), (realRank)])
        accuracy = TP / len(NTrainingAndTest[1]) * 1.0
        totalAcc += accuracy
        #print(TP)
        #print(totalAcc)
        #print(len(NTrainingAndTest[1]))
        final.append([totalAcc])
        forrest.append(final)
        class_tables.append(class_table)
    return forrest, class_tables

def get_key(item):
    return item[len(item) - 1][0]

def get_top_trees(forrest, M):
    od_forrest = sorted(forrest, key=get_key, reverse=True)
    return od_forrest[0:M]

def make_predict(forrest, test):
    predictions = []
    class_table = []
    accuracy = 0.0
    TP = 0.0
    for item in test:
        for tree in forrest:
            leaf = find_leaf(tree, item)
            probs = [leaf[len(leaf) - 2], leaf[len(leaf) - 1]]
            if probs[0] > probs[1]:
                prediction = 'yes'
            else:
                prediction = 'no'
            predictions.append(prediction)
        data = Counter(predictions)
        class_table.append([(prediction),(item[3])])
        #data.most_common()  # Returns all unique items and their counts
        #print(data.most_common(1)[0][0])  # Returns the highest occurring item
        if data.most_common(1)[0][0] == item[3]:
            TP += 1.0
    accuracy = TP / len(test) * 1.0
    return accuracy, class_table

def auto_make_predict(forrest, test):
    predictions = []
    accuracy = 0.0
    TP = 0.0
    class_table = []
    for item in test:
        for tree in forrest:
            leaf = find_auto_leaf(tree, item)
            prediction = leaf[len(leaf) - 1][0]
            realRank = item[0]
        if prediction == realRank:
            TP += 1.0
        class_table.append([(prediction), (realRank)])
        predictions.append(prediction)
        data = Counter(predictions)
        #data.most_common()  # Returns all unique items and their counts
        #print(data.most_common(1)[0][0])  # Returns the highest occurring item
        if data.most_common(1)[0][0] == item[0]:
            TP += 1.0
        class_table.append([(data.most_common(1)[0][0]), (realRank)])
    accuracy = TP / len(test) * 1.0
    return accuracy, class_table

def wisc_make_predict(forrest, test):
    predictions = []
    accuracy = 0.0
    TP = 0.0
    class_table = []
    for item in test:
        for tree in forrest:
            leaf = find_wisc_leaf(tree, item)
            probs = [leaf[len(leaf) - 2], leaf[len(leaf) - 1]]
            if probs[0] > probs[1]:
                prediction = 2
            else:
                prediction = 4
            predictions.append(prediction)
            realRank = item[9]
        if prediction == realRank:
            TP += 1.0
        data = Counter(predictions)
        #data.most_common()  # Returns all unique items and their counts
        #print(data.most_common(1)[0][0])  # Returns the highest occurring item
        if data.most_common(1)[0][0] == int(item[9]):
            TP += 1.0
        class_table.append([(data.most_common(1)[0][0]), (realRank)])

    accuracy = TP / len(test) * 1.0
    return accuracy, class_table


def get_best_index2(table, indexes):
    e_news = []
    for att in indexes:
        e_start = estart_car(table, 0)
        e_new = calc_enew(table, att, 0)
        e_news.append(e_start - e_new)
    return indexes[e_news.index(max(e_news))]

def estart_car(instances, index):
    empty = []
    count = 0.0
    for i in range(1,10):
        for spot in instances:
            if spot[index] == i:
                count +=1
        empty.append(count)
        count =0.0

    e_start = 0.0
    size = float(len(instances))
    for j in empty:
        if j == 0.0:
            break
        else:
            e_start += -((j/size) * log((j/size),2))
    return e_start

def DOE_class(mpg):
    mpg = float(mpg)
    guessRank = -1
    if mpg >= 45.0:
        guessRank = 10
    if  mpg < 45.0 and mpg >= 37.0:
        guessRank = 9
    if mpg < 37.0 and mpg >= 31.0:
        guessRank = 8
    if mpg < 31.0 and mpg >= 27.0:
        guessRank = 7
    if mpg < 27.0 and mpg >= 24.0:
        guessRank = 6
    if mpg < 24.0 and mpg >= 20.0:
        guessRank = 5
    if mpg < 20.0 and mpg >= 17.0:
        guessRank = 4
    if mpg < 17.0 and mpg >= 15.0:
        guessRank = 3
    if mpg < 15.0 and mpg >= 13.0:
        guessRank = 2
    if mpg < 13.0:
        guessRank = 1
    return guessRank

def weight_class(weight):
    weight = int(weight)
    guessWeight = -1
    if weight >= 3500:
        guessWeight = 5
    if weight <= 3499 and weight >= 3000:
        guessWeight = 4
    if weight <= 2999 and weight >= 2500:
        guessWeight = 3
    if weight <= 2499 and weight >= 2000:
        guessWeight = 2
    if weight <= 1999:
        guessWeight = 1
    return guessWeight

def change_mpg(instances, att_index):
    if att_index == 0:
        for row in instances:
            row[att_index] = DOE_class(row[att_index])
    else:
        for row in instances:
            row[att_index] = weight_class(row[att_index])
    return instances

def add_sum_col(table):
    #calculates total for tabulate data
    for row in table:
        row.append(sum_row(row))
    return table

def sum_row(row):
    # prints the sum of the rows without the first column
    return sum([row[i + 1] for i in range(len(row) - 1)])


def frequencies(table, index):
    """Returns a unique, sorted list of values in xs and occurrence counts for each value"""
    xs = sorted(get_column(table, index))
    values, counts = [], []
    for x in xs:
        if x not in values:
            values.append(x)
            counts.append(1)
        else:
            counts[-1] += 1
    return values, counts

def return_class(value):
    ind = np.argmax(value[1])
    return ind

def auto_confusion(table):
    # initializes the table
    confusion_table = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]
    for row in confusion_table:
        for i in range(10):
            row.append(0)

    #fill in table with prediction vs actual
    for row in table:
        actual = row[1]
        pred = row[0]
        confusion_table[actual-1][pred]+= 1

    #adds totals column
    add_sum_col(confusion_table)

    #adds recognition column
    i = 1
    for row in confusion_table:
        x = confusion_table[i-1][i]
        if x ==0 or row[11] == 0:
            row.append(0)
        else:
            reg = 100 * float(x)/float(row[11])
            row.append(reg)
        i += 1
    print ('===================================================================================')
    print ('Auto Data: Confusion Matrices')
    print ('===================================================================================')
    print ('Linear Regression(Stratified 10-fold Cross Validation Results):')
    print (tabulate(confusion_table, headers=["MPG", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "Total", "Recognition(%)"]))

def find_auto_leaf(tree, instance):
    scores = []
    for leaf in tree:
        score = 0.0
        if len(leaf) > 1:
            if leaf[0][1] == instance[1]:
                score += 1.0
            elif leaf[0][1] == 'NA':
                score += 0
            else:
                score -= 1.0
            if leaf[0][4] == instance[4]:
                score += 1.0
            elif leaf[0][4] == 'NA':
                score += 0
            else:
                score -= 1.0
            if leaf[0][6] == instance[6]:
                score += 1.0
            elif leaf[0][6] == 'NA':
                score += 0
            else:
                score -= 1.0
        scores.append(score)
    return tree[scores.index(max(scores))]

def get_auto_forest(table, N):
    forrest = []
    class_tables = []
    for i in range(0, N): #this loop holds the creation of the tree
        class_table = []
        totalAcc = 0.0
        TP = 0.0
        NTrainingAndTest = Stratified(table) #split up the table 2:1
        final = []
        atts2 = [1, 4, 6]
        if N == 1:
            final = split_best_tables2(NTrainingAndTest[0], atts2, [[3, 4, 5, 6, 8], [1, 2, 3, 4, 5], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]], final)  # produces a decision treee
        else:
            final = split_tables2(NTrainingAndTest[0], atts2, [[3, 4, 5, 6, 8], [1, 2, 3, 4, 5], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]], final) #produces a decision treee
        for row in final:
            val = frequencies(row, 0)
            x = return_class(val)
            row.append([val[0][x]])
        for item in NTrainingAndTest[1]:
            leaf = find_auto_leaf(final, item)
            prediction = leaf[len(leaf) - 1][0]
            realRank = item[0]
            if prediction == realRank:
                TP += 1.0
            class_table.append([(prediction), (realRank)])
        accuracy = TP / len(NTrainingAndTest[1]) * 1.0
        totalAcc += accuracy
        final.append([totalAcc])
        forrest.append(final)
        class_tables.append(class_table)
    return forrest, class_tables

def titanic_confussion(table):
    # initializes the table
    confusion_table = [['Yes'], ['No']]
    for row in confusion_table:
        for i in range(2):
            row.append(0)
    #fill in table with prediction vs actual
    for row in table:
        actual = row[1]
        pred = row[0]
        if actual == 'yes' and pred == 'yes':
            confusion_table[0][1]+=1
        if actual == 'yes' and pred == 'no':
            confusion_table[0][2] += 1
        if actual == 'no' and pred == 'yes':
            confusion_table[1][1] += 1
        if actual == 'no' and pred == 'no':
            confusion_table[1][2] += 1

    #adds totals column
    add_sum_col(confusion_table)

    #adds recognition column
    i = 1
    for row in confusion_table:
        x = confusion_table[i-1][i]
        if x ==0 or row[3] == 0:
            row.append(0)
        else:
            reg = 100 * float(x)/float(row[3])
            row.append(reg)
        i += 1
    print ('===================================================================================')
    print ('Titanic: Confusion Matrices')
    print ('===================================================================================')
    print (tabulate(confusion_table, headers=["Survived", "Yes", "No" "Total", "Recognition(%)"]))

def find_wisc_leaf(tree, instance):
    scores = []
    for leaf in tree:
        score = 0.0
        if len(leaf) > 1:
            for i in range(0, 9):
                if leaf[0][i] != 'NA':
                    if leaf[0][i] == instance[i]:
                        score += 1.0
                    else:
                        score -= 1.0
        scores.append(score)
    return tree[scores.index(max(scores))]



def get_wisc_forrest(table, N):
    forrest = []
    class_tables = []
    for i in range(0, N):  # this loop holds the creation of the tree
        class_table = []
        totalAcc = 0.0
        TP = 0.0
        NTrainingAndTest = Stratified(table)  # split up the table 2:1
        final = []
        if N == 1:
            final = split_best_tables_wisc(NTrainingAndTest[0], [0, 1, 2, 3, 4, 5, 6, 7, 8], \
                                      [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                       [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                       [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                       [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                       [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                       [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                       [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                       [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                       [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], final)  # produces a decision treee
        else:
            final = split_tables_wisc(NTrainingAndTest[0], [0, 1, 2, 3, 4, 5, 6, 7, 8], \
                                           [[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \
                                            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], final)  # produces a decision treee
        for row in final:
            countBen = 0.0
            countMal = 0.0
            for token in row:
                if len(token) > 2:
                    if int(token[9]) == 2:
                        countBen += 1.0
                    else:
                        countMal += 1.0
            row.append([countBen / (len(row) * 1.0)])
            row.append([countMal / (len(row) * 1.0)])
        for item in NTrainingAndTest[1]:
            leaf = find_leaf(final, item)
            probs = [leaf[len(leaf) - 2], leaf[len(leaf) - 1]]
            if probs[0] > probs[1]:
                prediction = 2
            else:
                prediction = 4
            realRank = item[9]
            # print(prediction + ' : ' + realRank)
            if prediction == realRank:
                TP += 1.0
            class_table.append([(prediction), (realRank)])
        accuracy = TP / len(NTrainingAndTest[1]) * 1.0
        totalAcc += accuracy
        final.append([totalAcc])
        forrest.append(final)
        class_tables.append(class_table)
    return forrest, class_tables

def wisconsin_confussion(table):
    # initializes the table
    confusion_table = [['Benign'], ['Malignant']]
    for row in confusion_table:
        for i in range(2):
            row.append(0)
    #fill in table with prediction vs actual
    for row in table:
        actual = int(row[1])
        pred = int(row[0])
        if actual == 2 and pred == 2:
            confusion_table[0][1]+=1
        if actual == 2 and pred == 4:
            confusion_table[0][2] += 1
        if actual == 4 and pred == 2:
            confusion_table[1][1] += 1
        if actual == 4 and pred == 4:
            confusion_table[1][2] += 1

    #adds totals column
    add_sum_col(confusion_table)

    #adds recognition column
    i = 1
    for row in confusion_table:
        x = confusion_table[i-1][i]
        if x ==0 or row[3] == 0:
            row.append(0)
        else:
            reg = 100 * float(x)/float(row[3])
            row.append(reg)
        i += 1
    print ('===================================================================================')
    print ('Wisconsin: Confusion Matrices')
    print ('===================================================================================')
    print (tabulate(confusion_table, headers=["Tumor Type", "Benign", "Malignant" "Total", "Recognition(%)"]))


def step_2(table):
    print('N = 7, M = 20, F = 2')
    print('Titanic Data:')
    firstTrainingAndTest = Stratified(table)

    forrest = get_forest(firstTrainingAndTest[0], 20)
    for row in forrest[1]:
        titanic_confussion(row)
    top_forrest = get_top_trees(forrest[0], 7)
    acc = make_predict(top_forrest, firstTrainingAndTest[1])
    titanic_confussion(acc[1])
    print('Accuracy of forrest: ' + str(acc[0]))

    normal = get_forest(firstTrainingAndTest[1], 1)
    acc2 = make_predict(normal[0], firstTrainingAndTest[1])
    titanic_confussion(acc[1])
    print('Accuracy of normal tree: ' + str(acc2[0]))

    print('Auto Data:')
    master = file_to_list('prof-auto-data.txt')
    masterTable1 = change_mpg(master, 0)
    masterTable = change_mpg(masterTable1, 4)
    firstTrainingAndTest = Stratified(masterTable)
    forrest2 = get_auto_forest(firstTrainingAndTest[0], 20)
    for row in forrest2[1]:
        auto_confusion(row)
    top_forrest = get_top_trees(forrest2[0], 7)
    acc = auto_make_predict(top_forrest, firstTrainingAndTest[1])
    auto_confusion(acc[1])
    print('Accuracy of forrest for auto: ' + str(acc[0]))

    normal = get_auto_forest(firstTrainingAndTest[1], 1)
    acc = auto_make_predict(normal[0], firstTrainingAndTest[1])
    auto_confusion(acc[1])
    print('Accuracy of normal tree for auto: ' + str(acc[0]))

    return forrest[1]

def step_3(table): #N = 10, M = 5, F = 2
    print('N = 10, M = 5, F = 2')
    print('Titanic Data:')
    firstTrainingAndTest = Stratified(table)
    forrest = get_forest(firstTrainingAndTest[0], 10)
    top_forrest = get_top_trees(forrest[0], 5)
    acc = make_predict(top_forrest, firstTrainingAndTest[1])
    print('Accuracy of forrest: ' + str(acc[0]))

    normal = get_forest(firstTrainingAndTest[1], 1)
    acc = make_predict(normal[0], firstTrainingAndTest[1])
    print('Accuracy of normal tree: ' + str(acc[0]))

    print('Auto Data:')
    master = file_to_list('prof-auto-data.txt')
    masterTable1 = change_mpg(master, 0)
    masterTable = change_mpg(masterTable1, 4)
    firstTrainingAndTest = Stratified(masterTable)

    forrest2 = get_auto_forest(firstTrainingAndTest[0], 10)
    top_forrest = get_top_trees(forrest2[0], 5)
    acc = auto_make_predict(top_forrest, firstTrainingAndTest[1])
    print('Accuracy of forrest for auto: ' + str(acc[0]))

    normal = get_auto_forest(firstTrainingAndTest[0], 1)
    acc = auto_make_predict(normal[0], firstTrainingAndTest[1])
    print('Accuracy of normal tree for auto: ' + str(acc[0]))
    return forrest[1]

def step_4(table):
    print('M = 3, N = 2, F = 4')
    print('Wisc data:')
    firstTrainingAndTest = Stratified(table)
    forrest = get_wisc_forrest(firstTrainingAndTest[0], 3)
    for row in forrest[1]:
        wisconsin_confussion(row)

    top_forrest = get_top_trees(forrest[0], 2)
    acc = wisc_make_predict(top_forrest, firstTrainingAndTest[1])
    wisconsin_confussion(acc[1])
    print('Accuracy of forrest for wisc: ' + str(acc[0]))

    normal = get_wisc_forrest(firstTrainingAndTest[0], 1)
    acc = wisc_make_predict(normal[0], firstTrainingAndTest[1])
    wisconsin_confussion(acc[1])
    print('Accuracy of normal tree for wisc: ' + str(acc[0]))


def main():
    tableMaster = read_csv('titanic.txt')
    #step_2(tableMaster)
    print('')
    step_3(tableMaster)
    print('')
    wisc = file_to_list('wisconsin.txt')
    #tep_4(wisc)

if __name__ == '__main__':
    main()
