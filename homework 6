import csv
import random
from random import randint
from tabulate import tabulate
from math import log
from collections import Counter


def read_csv(filename):
    the_file = open(filename, 'r')  # opens the file
    the_reader = csv.reader(the_file, dialect='excel')
    table = []  # makes an empty table
    for row in the_reader:
        if len(row) > 0:
            table.append(row)  # adds each row to the table
    the_file.close()  # closes the file
    return table  # returns the new table

def get_column(table, index):
    column = []  # creates a new table
    for row in table:
        column.append(row[index])
    return column

def attribute_frequencies(instances, att_index, class_index):
    # get unique list of attribute and class values
    att_vals = list(set(get_column(instances, att_index)))
    class_vals = list(set(get_column(instances, class_index)))
    # initialize the result
    result = {v: [{c: 0 for c in class_vals}, 0] for v in att_vals}  # build up the frequencies
    for row in instances:
        label = row[class_index]
        att_val = row[att_index]
        result[att_val][0][label] += 1
        result[att_val][1] += 1
    return result

def calc_estart(instances, att_index):
    size = len(instances)
    Yes = 0.0
    No = 0.0

    for row in instances:
        if row[att_index] == 'yes':
            Yes += 1
        else:
            No += 1

    p, q = Yes / size, No / size
    e_start = -(p * log(p, 2)) - (q * log(q, 2))

    return e_start


def calc_enew(instances, att_index, class_index):
    # get the length of the partition
    D = len(instances)
    # calculate the partition stats for att_index (see below)
    freqs = attribute_frequencies(instances, att_index, class_index)
    # find E_new from freqs (calc weighted avg)
    E_new = 0
    for att_val in freqs:
        D_j = float(freqs[att_val][1])
        probs = []
        for (_, c) in freqs[att_val][0].items():
            if c != 0:
                probs.append(c / D_j)
        #probs = [(c / D_j) for (_, c) in freqs[att_val][0].items()]
        E_D_j = -sum([p * log(p, 2) for p in probs])
        E_new += (D_j / D) * E_D_j
    return E_new


def get_best_index(table, indexes):
    e_news = []
    for att in indexes:
        e_start = calc_estart(table, 3)
        e_new = calc_enew(table, att, 3)
        #print('e_start is: ' + str(e_start) + ' e_new is: ' + str(e_new))
        e_news.append(e_start - e_new)
    return indexes[e_news.index(max(e_news))]

def group_by(table, att_index):
    # create unique list of grouping values
    grouping_values = list(set(get_column(table, att_index)))
    # create list of n empty partitions
    result = []
    for val in grouping_values:
        result.append([])
    # add rows to each partition
    for row in table:
        result[grouping_values.index(row[att_index])].append(row[:])
    return result

def random_attribute_subset(attributes, F):
    # shuffle and pick first F
    shuffled = attributes[:] # make a copy
    random.shuffle(shuffled)
    return shuffled[:F]


def split_tables(table, atts, final):
    if len(atts) < 1:
        return
    this_atts = atts[:]
    this_atts = random_attribute_subset(this_atts, 2)#get a random sub set of this_atts
    best = get_best_index(table, this_atts)
    this_atts.remove(best)
    grouped = group_by(table, best)
    for item in grouped:
        if len(atts) == 1: #this checks if it will be a leaf
            final.append(item)
    for item in grouped:
        split_tables(item, this_atts, final)
    return final

def split_best_tables(table, atts, final):
    if len(atts) < 1:
        return
    this_atts = atts[:]
    best = get_best_index(table, this_atts)
    this_atts.remove(best)
    grouped = group_by(table, best)
    for item in grouped:
        if len(atts) == 1: #this checks if it will be a leaf
            final.append(item)
    for item in grouped:
        split_best_tables(item, this_atts, final)
    return final

def Stratified(table):
    randomized = table[:]  # this will randomize the table we passed in
    n = len(table)
    for i in range(n):
        j = randint(0, n - 1)
        randomized[i], randomized[j] = randomized[j], randomized[i]  # changes all the positions of the item in the list

    k = 3
    empty = [[] for i in range(k)]  # the empty 10-folds
    count = 0
    temp = randomized
    count = 0
    for row in table:
        if count == k:
            count = 0
        empty[count].append(row)
        count += 1
    training_set = empty[0] + empty[1]
    return [training_set, empty[2]]

def get_forest(table, N):
    forrest = []
    class_tables = []
    for i in range(0, N): #this loop holds the creation of the tree
        class_table = []
        totalAcc = 0.0
        TP = 0.0
        NTrainingAndTest = Stratified(table) #split up the table 2:1
        final = []
        if N == 1:
            final = split_best_tables(NTrainingAndTest[0], [0, 1, 2], final)  # produces a decision treee
        else:
            final = split_tables(NTrainingAndTest[0], [0, 1, 2], final) #produces a decision treee
        for row in final:
            countYes = 0.0
            countNo = 0.0
            for token in row:
                if len(token) > 2:
                    if token[3] == 'yes':
                        countYes += 1.0
                    else:
                        countNo += 1.0
            row.append([countYes / (len(row) * 1.0)])
            row.append([countNo / (len(row) * 1.0)])
        for item in NTrainingAndTest[1]:
            for row in final:
                prediction = 'yes' # the default is no
                for instance in row:
                    if len(instance) > 2:
                        if instance[0] == item[0] and instance[1] == item[1] and instance[2] == item[2]:
                            probs = [row[len(row) - 2], row[len(row) - 1]]
                            if probs[0] > probs[1]:
                                prediction = 'yes'
                            else:
                                prediction = 'no'
            realRank = item[3]
            # print(prediction + ' : ' + realRank)
            if prediction == realRank:
                TP += 1.0
            class_table.append([(prediction), (realRank)])
        accuracy = TP / len(NTrainingAndTest[1]) * 1.0
        totalAcc += accuracy
        #print(TP)
        #print(totalAcc)
        #print(len(NTrainingAndTest[1]))
        final.append([totalAcc])
        forrest.append(final)
        class_tables.append(class_table)
    return forrest, class_tables

def get_key(item):
    return item[len(item) - 1][0]

def get_top_trees(forrest, M):
    od_forrest = sorted(forrest, key=get_key, reverse=True)
    return od_forrest[0:M]

def make_predict(forrest, test):
    predictions = []
    accuracy = 0.0
    TP = 0.0
    for item in test:
        for tree in forrest:
            for row in tree:
                prediction = 'yes'  # the default is no
                if len(row) > 2:
                    for instance in row:
                        if len(instance) > 2:
                            if instance[0] == item[0] and instance[1] == item[1] and instance[2] == item[2]:
                                probs = [row[len(row) - 2], row[len(row) - 1]]
                                if probs[0] > probs[1]:
                                    prediction = 'yes'
                                else:
                                    prediction = 'no'
            # print(prediction + ' : ' + realRank)
            predictions.append(prediction)
        data = Counter(predictions)
        #data.most_common()  # Returns all unique items and their counts
        #print(data.most_common(1)[0][0])  # Returns the highest occurring item
        if data.most_common(1)[0][0] == item[3]:
            TP += 1.0
    accuracy = TP / len(test) * 1.0
    return accuracy

def step_2(table):
    firstTrainingAndTest = Stratified(table)
    forrest = get_forest(firstTrainingAndTest[1], 20)
    top_forrest = get_top_trees(forrest[0], 7)
    acc = make_predict(top_forrest, firstTrainingAndTest[1])
    print('Accuracy of forrest: ' + str(acc))

    normal = get_forest(firstTrainingAndTest[1], 1)
    acc = make_predict(normal[0], firstTrainingAndTest[1])
    print('Accuracy of normal tree: ' + str(acc))

def main():
    tableMaster = read_csv('titanic.txt')
    ''''
    firstTrainingAndTest = Stratified(tableMaster)
    final = []
    final = split_tables(firstTrainingAndTest[0], [0, 1, 2], final)
    print('')
    forrest = get_forest(firstTrainingAndTest[1], 3)
    top_forrest = get_top_trees(forrest, 2)
    make_predict(top_forrest, firstTrainingAndTest[1])
    '''
    step_2(tableMaster)


if __name__ == '__main__':
    main()
